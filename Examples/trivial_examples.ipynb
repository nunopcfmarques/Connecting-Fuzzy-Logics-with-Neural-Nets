{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.Algorithms.ReLUToTLogic import *\n",
    "from src.Algorithms.TLogicToReLU import *\n",
    "from src.Logic.Solver import *\n",
    "\n",
    "sys.setrecursionlimit(10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple examples jupyter notebook\n",
    "\n",
    "This notebook illustrate how to properly use the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR(0, 0) = tensor([0.], dtype=torch.float64, grad_fn=<ViewBackward0>)\n",
      "XOR(0, 1) = tensor([1.], dtype=torch.float64, grad_fn=<ViewBackward0>)\n",
      "XOR(1, 0) = tensor([1.], dtype=torch.float64, grad_fn=<ViewBackward0>)\n",
      "XOR(1, 1) = tensor([0.], dtype=torch.float64, grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "XOR_ReLU = ReLUNetwork(\n",
    "    weights=[\n",
    "        torch.tensor([[1, -1], [-1, 1]], dtype=torch.float64),\n",
    "        torch.tensor([[1, 1]], dtype=torch.float64)\n",
    "    ],\n",
    "    biases=[\n",
    "        torch.tensor([0, 0], dtype=torch.float64),\n",
    "        torch.tensor([0], dtype=torch.float64)\n",
    "    ]\n",
    ")\n",
    "\n",
    "XOR_ReLU.construct_layers()\n",
    "\n",
    "# Let's check the output of the network against the truth table\n",
    "for x1 in [0, 1]:\n",
    "    for x2 in [0, 1]:\n",
    "        x = torch.tensor([x1, x2], dtype=torch.float64)\n",
    "        print(f\"XOR({x1}, {x2}) = {XOR_ReLU.forward(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the formula for this network. It is well know that the formula $((x_1∧(¬x_2))\\lor((¬x_1)∧x_2))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((x2⊙(¬x1))⊕(x1⊙(¬x2)))\n",
      "XOR(0, 0) = 0.0\n",
      "XOR(0, 0) = 0.0\n",
      "XOR(0, 1) = 1.0\n",
      "XOR(0, 1) = 1.0\n",
      "XOR(1, 0) = 1.0\n",
      "XOR(1, 0) = 1.0\n",
      "XOR(1, 1) = 0.0\n",
      "XOR(1, 1) = 0.0\n"
     ]
    }
   ],
   "source": [
    "CReLU = transform_ReLU_to_CReLU(XOR_ReLU)\n",
    "\n",
    "transformed_formula = CReLU_to_formula(CReLU)[0]\n",
    "original_formula = \"((x1∧(¬x2))V((¬x1)∧x2))\"\n",
    "\n",
    "print(transformed_formula)\n",
    "\n",
    "# Let's check the output of the network against the truth table\n",
    "prsr = Parser(Lukasiewicz())\n",
    "for x1 in [0, 1]:\n",
    "    for x2 in [0, 1]:\n",
    "        transformed_formula_ast = prsr.generate_ast_with_degs(transformed_formula)[0]\n",
    "        original_formula_ast = prsr.generate_ast_with_degs(original_formula)[0]\n",
    "\n",
    "\n",
    "        print(f\"XOR({x1}, {x2}) = {Parser.evaluate_formula(prsr, transformed_formula_ast, {'x1': x1, 'x2': x2})}\")\n",
    "        print(f\"XOR({x1}, {x2}) = {Parser.evaluate_formula(prsr, original_formula_ast, {'x1': x1, 'x2': x2})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the formulas are equivalent in Lukasiewicz logic. Let's now check for reachability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, [x2 = 0, x1 = 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SolveFormulaSMT(transformed_formula, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the formula is solvable (something we already knew), and we are given a correct model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things that would be fun to check are the formulas of the neurons and check if we can learn anything from them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer formulas:\n",
      "(x1⊙(¬x2))\n",
      "(x2⊙(¬x1))\n",
      "\n",
      "Second layer formulas:\n",
      "((x2⊙(¬x1))⊕(x1⊙(¬x2)))\n"
     ]
    }
   ],
   "source": [
    "_, neuron_formulas = CReLU_to_formula(CReLU)\n",
    "\n",
    "first_layer_formulas = neuron_formulas[0]\n",
    "second_layer_formulas = neuron_formulas[1]\n",
    "\n",
    "print(\"First layer formulas:\")\n",
    "print(first_layer_formulas[1])\n",
    "print(first_layer_formulas[2] + \"\\n\")\n",
    "print(\"Second layer formulas:\")\n",
    "print(second_layer_formulas[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the first formula is only true when $x_1$ is true and $x_2$ is false, which is the case $(1,0)$. As for the second formula, it is only true when $x_1$ is false and $x_2$ is true, which is the case $(0,1)$.\n",
    "\n",
    "The final layer computes the \"OR\" part of XOR, being true on input $(1,0)$ or $(0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, using this algorithm, we can explain exactly what each neuron outputs in this simple example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
